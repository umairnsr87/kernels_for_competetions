{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'train.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"../input/train.csv\")\n",
    "test=pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the train is(42000, 785)\n",
      "the shape of the test is(28000, 784)\n"
     ]
    }
   ],
   "source": [
    "#checking the shape of the train and test dataset\n",
    "\n",
    "print(\"the shape of the train is\"+str(train.shape))\n",
    "\n",
    "print(\"the shape of the test is\"+str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of N/A entries in the train dataset are 0\n",
      "The number of N/A entries in the test dataset are 0\n"
     ]
    }
   ],
   "source": [
    "#checking any n/a or null values in the dataset\n",
    "print(\"The number of N/A entries in the train dataset are\"+\" \"+str(train.isna().sum().sum()))\n",
    "print(\"The number of N/A entries in the test dataset are\"+\" \"+str(test.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this dataset is neat and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns[:784:785]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing the tensorflow and keras sequential model\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saperating the training data and their labels\n",
    "x_train=train.drop([\"label\"],axis=1)\n",
    "y_train=train[\"label\"]\n",
    "#now checking the shape of saperated set\n",
    "x_train.shape,y_train.shape\n",
    "#delting the train data now as we do not need tis data\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the data for faster processing \n",
    "x_train=x_train/255.0\n",
    "test=test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the train is(42000, 28, 28, 1)\n",
      "the shape of the test is(28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#changing the shape of the dataset to feed it to keras model\n",
    "#1 is representing that the image is grayscale\n",
    "x_train=x_train.values.reshape(x_train.shape[0],28,28,1)\n",
    "test=test.values.reshape(test.shape[0],28,28,1)\n",
    "#now checking the shape after reshaping\n",
    "print(\"the shape of the train is\"+str(x_train.shape))\n",
    "\n",
    "print(\"the shape of the test is\"+str(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will be converting the y_train into the format which keras understand i.e. One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.utils import to_categorical\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train=to_categorical(y_train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37800, 28, 28, 1), (37800, 10), (4200, 28, 28, 1), (4200, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now splitting the data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train1,x_val,y_train1,y_val=train_test_split(x_train,y_train,test_size=.10,random_state=42)\n",
    "x_train1.shape,y_train1.shape,x_val.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now setting the cnn model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv2d layer->conv2d laYer->poolin2d->Dropout->Flatten->Dense ->Dropout->Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "#first conv2d layer\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\",input_shape=(28,28,1)))\n",
    "#second-con2D layer\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(10,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x7fc96f1e8e10>>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will define the optimizer function I am using SGD\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer=SGD(.01),\n",
    "             metrics=[\"accuracy\"])\n",
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs1 = 30 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size1 = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37800 samples, validate on 4200 samples\n",
      "Epoch 1/30\n",
      " - 5s - loss: 1.1216 - acc: 0.6507 - val_loss: 0.3570 - val_acc: 0.8950\n",
      "Epoch 2/30\n",
      " - 3s - loss: 0.4454 - acc: 0.8624 - val_loss: 0.2592 - val_acc: 0.9262\n",
      "Epoch 3/30\n",
      " - 3s - loss: 0.3593 - acc: 0.8904 - val_loss: 0.2241 - val_acc: 0.9364\n",
      "Epoch 4/30\n",
      " - 3s - loss: 0.3079 - acc: 0.9054 - val_loss: 0.1911 - val_acc: 0.9438\n",
      "Epoch 5/30\n",
      " - 3s - loss: 0.2753 - acc: 0.9169 - val_loss: 0.1737 - val_acc: 0.9498\n",
      "Epoch 6/30\n",
      " - 3s - loss: 0.2532 - acc: 0.9237 - val_loss: 0.1590 - val_acc: 0.9531\n",
      "Epoch 7/30\n",
      " - 3s - loss: 0.2301 - acc: 0.9310 - val_loss: 0.1501 - val_acc: 0.9564\n",
      "Epoch 8/30\n",
      " - 3s - loss: 0.2178 - acc: 0.9354 - val_loss: 0.1336 - val_acc: 0.9612\n",
      "Epoch 9/30\n",
      " - 3s - loss: 0.2012 - acc: 0.9402 - val_loss: 0.1232 - val_acc: 0.9624\n",
      "Epoch 10/30\n",
      " - 3s - loss: 0.1866 - acc: 0.9436 - val_loss: 0.1193 - val_acc: 0.9631\n",
      "Epoch 11/30\n",
      " - 3s - loss: 0.1738 - acc: 0.9484 - val_loss: 0.1102 - val_acc: 0.9655\n",
      "Epoch 12/30\n",
      " - 3s - loss: 0.1634 - acc: 0.9503 - val_loss: 0.1094 - val_acc: 0.9660\n",
      "Epoch 13/30\n",
      " - 3s - loss: 0.1548 - acc: 0.9551 - val_loss: 0.1021 - val_acc: 0.9671\n",
      "Epoch 14/30\n",
      " - 3s - loss: 0.1469 - acc: 0.9555 - val_loss: 0.0955 - val_acc: 0.9695\n",
      "Epoch 15/30\n",
      " - 3s - loss: 0.1380 - acc: 0.9605 - val_loss: 0.0929 - val_acc: 0.9698\n",
      "Epoch 16/30\n",
      " - 3s - loss: 0.1339 - acc: 0.9603 - val_loss: 0.0875 - val_acc: 0.9729\n",
      "Epoch 17/30\n",
      " - 3s - loss: 0.1247 - acc: 0.9630 - val_loss: 0.0849 - val_acc: 0.9731\n",
      "Epoch 18/30\n",
      " - 3s - loss: 0.1208 - acc: 0.9637 - val_loss: 0.0835 - val_acc: 0.9743\n",
      "Epoch 19/30\n",
      " - 3s - loss: 0.1161 - acc: 0.9658 - val_loss: 0.0801 - val_acc: 0.9748\n",
      "Epoch 20/30\n",
      " - 3s - loss: 0.1137 - acc: 0.9662 - val_loss: 0.0770 - val_acc: 0.9745\n",
      "Epoch 21/30\n",
      " - 3s - loss: 0.1094 - acc: 0.9669 - val_loss: 0.0754 - val_acc: 0.9743\n",
      "Epoch 22/30\n",
      " - 3s - loss: 0.1062 - acc: 0.9685 - val_loss: 0.0740 - val_acc: 0.9762\n",
      "Epoch 23/30\n",
      " - 3s - loss: 0.1036 - acc: 0.9685 - val_loss: 0.0706 - val_acc: 0.9779\n",
      "Epoch 24/30\n",
      " - 3s - loss: 0.0974 - acc: 0.9704 - val_loss: 0.0678 - val_acc: 0.9788\n",
      "Epoch 25/30\n",
      " - 3s - loss: 0.0923 - acc: 0.9723 - val_loss: 0.0677 - val_acc: 0.9798\n",
      "Epoch 26/30\n",
      " - 3s - loss: 0.0915 - acc: 0.9721 - val_loss: 0.0645 - val_acc: 0.9798\n",
      "Epoch 27/30\n",
      " - 3s - loss: 0.0913 - acc: 0.9722 - val_loss: 0.0631 - val_acc: 0.9807\n",
      "Epoch 28/30\n",
      " - 3s - loss: 0.0861 - acc: 0.9736 - val_loss: 0.0625 - val_acc: 0.9807\n",
      "Epoch 29/30\n",
      " - 3s - loss: 0.0838 - acc: 0.9735 - val_loss: 0.0615 - val_acc: 0.9814\n",
      "Epoch 30/30\n",
      " - 3s - loss: 0.0827 - acc: 0.9743 - val_loss: 0.0594 - val_acc: 0.9824\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train1, y_train1, batch_size = batch_size1, epochs = epochs1, \n",
    "         validation_data = (x_val, y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prducing the results before the augmentation\n",
    "# predict results\n",
    "#results = model.predict(test)\n",
    "#selecting the final probability as this will give you the probability of the numbers\n",
    "#Final_pred = results.argmax(axis=1)\n",
    "#creating the csv file for the submission\n",
    "#Images=np.arange(1,28001)\n",
    "#submission=pd.DataFrame([Images,Final_pred]).T\n",
    "#submission.rename({0:\"ImageId\",1:\"Label\"})\n",
    "#submission.to_csv(\"Mnist_Kaggle_submission.csv\",header=[\"ImageId\",\"Label\"],index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will do data augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=10,\n",
    "    zoom_range=.1,\n",
    "    width_shift_range=.1,\n",
    "    height_shift_range=.1,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False)\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 900s - loss: 0.1594 - acc: 0.9518 - val_loss: 0.0323 - val_acc: 0.9912\n",
      "Epoch 2/30\n",
      " - 903s - loss: 0.0877 - acc: 0.9735 - val_loss: 0.0271 - val_acc: 0.9926\n",
      "Epoch 3/30\n",
      " - 897s - loss: 0.0694 - acc: 0.9790 - val_loss: 0.0223 - val_acc: 0.9926\n",
      "Epoch 4/30\n",
      " - 893s - loss: 0.0595 - acc: 0.9820 - val_loss: 0.0232 - val_acc: 0.9921\n",
      "Epoch 5/30\n",
      " - 899s - loss: 0.0528 - acc: 0.9840 - val_loss: 0.0244 - val_acc: 0.9926\n",
      "Epoch 6/30\n",
      " - 888s - loss: 0.0479 - acc: 0.9853 - val_loss: 0.0233 - val_acc: 0.9938\n",
      "Epoch 7/30\n",
      " - 881s - loss: 0.0440 - acc: 0.9865 - val_loss: 0.0212 - val_acc: 0.9931\n",
      "Epoch 8/30\n",
      " - 887s - loss: 0.0407 - acc: 0.9875 - val_loss: 0.0242 - val_acc: 0.9926\n",
      "Epoch 9/30\n",
      " - 896s - loss: 0.0383 - acc: 0.9882 - val_loss: 0.0240 - val_acc: 0.9933\n",
      "Epoch 10/30\n",
      " - 892s - loss: 0.0363 - acc: 0.9887 - val_loss: 0.0224 - val_acc: 0.9940\n",
      "Epoch 11/30\n",
      " - 889s - loss: 0.0342 - acc: 0.9894 - val_loss: 0.0213 - val_acc: 0.9943\n",
      "Epoch 12/30\n",
      " - 889s - loss: 0.0327 - acc: 0.9898 - val_loss: 0.0199 - val_acc: 0.9940\n",
      "Epoch 13/30\n",
      " - 890s - loss: 0.0311 - acc: 0.9902 - val_loss: 0.0217 - val_acc: 0.9940\n",
      "Epoch 14/30\n",
      " - 889s - loss: 0.0297 - acc: 0.9907 - val_loss: 0.0186 - val_acc: 0.9955\n",
      "Epoch 15/30\n",
      " - 902s - loss: 0.0284 - acc: 0.9910 - val_loss: 0.0198 - val_acc: 0.9940\n",
      "Epoch 16/30\n",
      " - 899s - loss: 0.0273 - acc: 0.9914 - val_loss: 0.0202 - val_acc: 0.9945\n",
      "Epoch 17/30\n",
      " - 892s - loss: 0.0265 - acc: 0.9916 - val_loss: 0.0208 - val_acc: 0.9945\n",
      "Epoch 18/30\n",
      " - 895s - loss: 0.0254 - acc: 0.9919 - val_loss: 0.0200 - val_acc: 0.9943\n",
      "Epoch 19/30\n",
      " - 901s - loss: 0.0249 - acc: 0.9921 - val_loss: 0.0213 - val_acc: 0.9945\n",
      "Epoch 20/30\n",
      " - 894s - loss: 0.0237 - acc: 0.9924 - val_loss: 0.0198 - val_acc: 0.9940\n",
      "Epoch 21/30\n",
      " - 897s - loss: 0.0230 - acc: 0.9927 - val_loss: 0.0195 - val_acc: 0.9950\n",
      "Epoch 22/30\n",
      " - 902s - loss: 0.0222 - acc: 0.9929 - val_loss: 0.0187 - val_acc: 0.9952\n",
      "Epoch 23/30\n",
      " - 903s - loss: 0.0217 - acc: 0.9930 - val_loss: 0.0218 - val_acc: 0.9943\n",
      "Epoch 24/30\n",
      " - 904s - loss: 0.0211 - acc: 0.9932 - val_loss: 0.0206 - val_acc: 0.9945\n",
      "Epoch 25/30\n",
      " - 894s - loss: 0.0204 - acc: 0.9935 - val_loss: 0.0188 - val_acc: 0.9948\n",
      "Epoch 26/30\n",
      " - 892s - loss: 0.0201 - acc: 0.9936 - val_loss: 0.0212 - val_acc: 0.9945\n",
      "Epoch 27/30\n",
      " - 896s - loss: 0.0193 - acc: 0.9938 - val_loss: 0.0198 - val_acc: 0.9943\n",
      "Epoch 28/30\n",
      " - 895s - loss: 0.0190 - acc: 0.9939 - val_loss: 0.0207 - val_acc: 0.9943\n",
      "Epoch 29/30\n",
      " - 896s - loss: 0.0186 - acc: 0.9940 - val_loss: 0.0211 - val_acc: 0.9940\n",
      "Epoch 30/30\n",
      " - 896s - loss: 0.0181 - acc: 0.9942 - val_loss: 0.0199 - val_acc: 0.9948\n"
     ]
    }
   ],
   "source": [
    "final_model=model.fit_generator(datagen.flow(x_train1,y_train1,batch_size=batch_size1),\n",
    "                   epochs=epochs1,\n",
    "                   validation_data=(x_val,y_val),\n",
    "                   verbose=2,\n",
    "                   steps_per_epoch=x_train1.shape[0],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(test)\n",
    "#selecting the final probability as this will give you the probability of the numbers\n",
    "Final_pred = results.argmax(axis=1)\n",
    "#creating the csv file for the submission\n",
    "Images=np.arange(1,28001)\n",
    "submission=pd.DataFrame([Images,Final_pred]).T\n",
    "submission.rename({0:\"ImageId\",1:\"Label\"})\n",
    "submission.to_csv(\"Mnist_Kaggle_submission.csv\",header=[\"ImageId\",\"Label\"],index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
